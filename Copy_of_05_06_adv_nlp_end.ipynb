{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gstripling/Generative_AI/blob/main/Copy_of_05_06_adv_nlp_end.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine Tuning a LLM"
      ],
      "metadata": {
        "id": "ZRbJve_-LSit"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine-Tune using Keras"
      ],
      "metadata": {
        "id": "LHyCJot1_ywD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gv2JMXOWAlRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "model = TFGPT2LMHeadModel.from_pretrained('gpt2')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Add the EOS token as the pad token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load your domain-specific data\n",
        "train_data_path = '/content/advertising.csv'\n",
        "with open(train_data_path, 'r', encoding='utf-8') as file:\n",
        "    train_texts = file.readlines()\n",
        "\n",
        "# Tokenize and encode the texts\n",
        "input_ids = tokenizer(train_texts, return_tensors='tf', padding=True, truncation=True)\n",
        "\n",
        "# Shift the input sequence to create target sequence\n",
        "labels = tf.roll(input_ids['input_ids'], shift=-1, axis=-1)\n",
        "\n",
        "# Create TensorFlow Dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((dict(input_ids), labels))\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 8\n",
        "num_epochs = 3\n",
        "learning_rate = 5e-5\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE),\n",
        "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_dataset.shuffle(1000).batch(batch_size),\n",
        "          epochs=num_epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFqo75c3Alaj",
        "outputId": "1e6352d2-7ae8-4354-d7d7-5ffcf36b56a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "126/126 [==============================] - 66s 183ms/step - loss: 2.2448 - accuracy: 0.5805\n",
            "Epoch 2/3\n",
            "126/126 [==============================] - 23s 183ms/step - loss: 1.6609 - accuracy: 0.6348\n",
            "Epoch 3/3\n",
            "126/126 [==============================] - 23s 184ms/step - loss: 1.5474 - accuracy: 0.6511\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78b1604b44c0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ]
}